# -*- coding: utf-8 -*-
"""Risco_financeiro_ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0j0ZVKwlm6RDRFyQVb1WGlcSlhWKaDT
"""

# Para manipulacao dos dados
import pandas as pd
import numpy as np

# para graficos
import seaborn as sns
import matplotlib.pyplot as plt
!pip install sweetviz
import sweetviz as sv


# para modelagem
from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid, KFold
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder


#### Settings ####
pd.set_option("max_colwidth", 1000)
pd.set_option("max_rows", 20)
pd.set_option("max_columns", 1000)
pd.set_option("precision", 2)
pd.options.display.float_format = "{:,.2f}".format
plt.style.use("classic")
# %load_ext nb_black

"""# **2. PRIMEIRAS IMPRESSÕES**"""

# Carregar Dataset

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/german_credit_data.csv", index_col= 0)#, sep=';', decimal= ',' )

# Ver as primeiras linhas

df.head()

# Ver as ultimas linhas
df.tail()

# Ver Qualquer quantidade de linahs no final ou inicio

# df.head(20)
# df.tail(20)

# Informação das variaveis ex:( tipos, valores nulos, memoria que o onjeto ta consumido, etc.)

df.info() # Ver variaveis

# Comos aber o numero de colunas e linhas

df.shape

# Criar nova linha de codigo Ctrl + M B

# f-string

print(f'O dataset possui {df.shape[0]} linhas e {df.shape[1]} colunas')

# Como acessar os valores de Shape.
df.shape[1]

# Usar f-string dentro de Quarys

"""
import pandasql as ps

filtro_idade = 70
quary = f"""
# select = from df
# where age < {filtro_idade} 
"""

ps.sqldf(query, locals())"""

# Acessar elementos: loc e iloc

df.iloc[0:4, 0:7] # acessando intervalo linha 0 a 3, e coluna 0 a 7

# Outras formas de acessar elementos, usando nome da coluna

df.loc[0:3, "Age"]

# Se quiser trazer mais de uma coluna

df.loc[0:3, ['Age', 'Sex', 'Job']] 

# lembrar de adicionar [] a uma lista se for mais de uma coluna

# Acessar pelo nome da coluna como indice

novo_dataframe = df[["Age", "Housing"]]
novo_dataframe.head()

# Estatistica Descritiva // IMPORTANTE

df.describe()

# Média de duração = 20,90
# Desvio padrão = 4,00 á 72,00
# Maximo de idade = 75 anos
# Minimo de diade = 18 anos

# Estatistica descritiva por grupo

df.groupby('Sex')[['Age', 'Credit amount', 'Duration']].mean()

# Cria duas colunas de Sexo Masculino e Feminino, por as demais colunas. // max/ min/ med

# Estatistica por grupo por motivo

df.groupby('Purpose')[['Age', 'Credit amount', 'Duration']].mean()

# Dados que estão faltando na tabela // Missing

df.isna()

# IMPORTANTE AVALIAR O DATASET com muitos missings

# Total de missing por coluna // Missings Ex

df.isna().sum()

# Media de dados faltantes por coluna Ex

df.isna().mean() # Caso queira preencher, poderia usar fillna()

# Usar fillna() apenas depois de teste e treino para evitar erro.

# Encontrar variaveis(colunas) categoricas

df.Housing.value_counts() # conta quantidade por cada tipo de valor

# Encontrar categoria por sexo Ex

df.Sex.value_counts() # Mais homens que mulheres

# Encontrar categoria por percentual

df.Sex.value_counts(normalize=True).sort_index() * 100

# Trazer todos os nomes das colunas

df.columns

# Filtrar(Query para Lógica)

df.query('Age > 70') # filtra  todos os individuos com 70 anos

# Filtrar com dois parametros

homens_idosos = df.query('Age > 70 & Sex == "male" ' ) # Criando um novo dataframe
homens_idosos.head()

# cuidado com letras maiusculas e minusculas, porque mudam totalmente o grafico

# Método para tentar inferir se a variavel é categorica

provavel_categorica = {}

for var in df.columns:
  provavel_categorica[var] = 1.0 * df[var].nunique()/df[var].count() < 0.03
provavel_categorica # mostra o que é categorico.

# IMPORTANTE PARA DATASETS MUITO GRANDES.
# Essa verificação mostra a quantidade de varivaeis dentro de uma coluna
# Se for menor que 5 e  interessante usar para a modelagem
# Se não será preciso ser feito de outra forma.

# Dataset que irá ser usado no modelo.

df['Risco_cliente'] = np.where(df['Risk'] == "bad", 1, 0) # Cria coluna
df.drop('Risk', axis=1, inplace=True) # Dropa coluna // inplace torna  permanente
df.head()

# Condição de Verdadeiro ou Falso

# Ver porcentagem

df.Risco_cliente.value_counts(normalize=True) * 100

"""**3. Análise Exploratória**

PLOT COM PANDAS
"""

# Mostrando grafico com dados do pandas no matplotlib

_ = plt.figure(figsize=(8, 4)) # largura e altura das barras
_ = df.Risco_cliente.value_counts().plot(kind='bar') # Colcoando dados em grafico de barras.
_ = plt.title('N° de cliente por tipo de risco') # Titulo
_ = plt.ylim(0, 800) # Altura e largura

# Media de risco por idade

_ = df.groupby('Risco_cliente')['Age'].mean().plot(kind='bar')
_ = plt.title('Idade media por tipo de risco')



"""**PLOT COM MATPLOTLIB ( FOCO NA HIERARQUIA)**"""

# GRAFICO SENDO CONSTRUIDO POR PARTES

_= plt.figure(figsize=(6, 4)) # Criando a imagem que vai receber o grafico
_ = plt.xlabel('Idade') # Constuindo eixo x
_= plt.ylabel('Quantidade') # Construindo eixo y

# Adicionando elementos na figura

_ = plt.figure(figsize=(7,4)) # Criando a imagem que vai receber o grafico
_ = plt.hist(data=df,x='Age', bins=20, rwidth=0.9, color='Red') # Histograma / Dataset / dados em X / Tamanho do retanguilo / largura do retangulo / Colocar cor
_ = plt.xlabel('Idade') # Nome do eixo X
_ = plt.title('Histograma para idade') # Titulo do grafico

# _ = plt.ylabel('')
# Pode ser adicionado a tabela seguindo padrão de x

"""**PLOT COM SEABORN**"""

_ = sns.relplot(x='Age', y = 'Credit amount', data=df) # Montar grafico / eixo X / eixo Y / dataset

# Relação de credito e montante por Risco de cliente

_ = sns.relplot(x='Duration', # Montar grafico / eixo X
                y = 'Credit amount', # eixo Y
                hue='Risco_cliente', # mudar cor de acordo com a variação do dado
                data=df) # dataset

# Como adicionar uma plaeta de cores e o tamanho dos dados

_ = sns.relplot(x='Duration', 
                y = 'Credit amount', 
                hue='Risco_cliente', 
                palette=['purple', 'blue'], # Paleta de cores no grafico
                size='Age', # aumenta tamanho de acordo com a idade
                data=df)

# Grafico de contagem por Categoria

_ = sns.catplot(x='Housing', kind='count', data=df) # usando grafico categorico

# Grafico de contagem por Categoria Ex: 2

_ = sns.catplot(x='Purpose', kind='count', data=df)

# espere

"""**DASHBOARD AUTOMATIZADO**"""

# Dashboard com duas linhas / Usando SWEETVIZ

reporte = sv.analyze(df) # Fazer analise do grafico
reporte.show_html()

# Para abrir o arquivo SWEETVIZ_REPORT procure nos arquivo a direita.
# Se clicar em um grafico ele trava

"""## **## 4. MODELAGEM DE MACHINE LEARNING: RANDOM FOREST**"""

# lISTANDO AS COLUNA DE DADOS QUE IREMOS SUAR

features = ['Age', 'Job', 'Credit amount', 'Purpose', 'Housing', 'Duration']
target = 'Risco_cliente'

# Divisão mde dados para treinamento

from sklearn.model_selection import train_test_split # separar por treino e teste

X = df[features] # colunas de treino
y = df[target] # Colunas de teste

# função de treino e teste

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.50, random_state = 999
)
# teste_size = 25%(Será separado em uma porcentagem, nesse caso) // Random entre 999

# Tratando as variaveis categoricas (cuidado quando for numerica)

cat_features = ['Job', 'Housing', 'Purpose'] # Categorica
num_features = ['Age', 'Credit amount', 'Duration'] # nuemricas

# Instalar o pacote de transformação de categoria em binario

!pip install category_encoders
import category_encoders as ce # Transforma colunas categoricas em binarias

encoder = ce.OneHotEncoder(cols=cat_features) # Como transformar

X_train = encoder.fit_transform(X_train) 
# Fazendo tratamento para treinar a rede neural

X_train.head() # Ver base do treino

# aplicando trataemntopara teste na rede neural e como ver

X_test = encoder.transform(X_test)

X_test.head() # Visualizar

# Treinando a rede neural

clf_RF = RandomForestClassifier() # Cria o objeto
clf_RF.fit(X_train, y_train) # Aplica o treino nas variaveis



# Aplciar o teste

y_pred = clf_RF.predict(X_test) # Executando o teste

y_pred # Mostrar resultado do teste



# Porcentagem de acertos no codigo

from sklearn.metrics import accuracy_score

print("Acurácia: " + str(accuracy_score(y_test, y_pred)))  # quantidade de acertos do codigo

from sklearn.metrics import f1_score

print(
    "F1 Score: {}".format(f1_score(y_test, y_pred))
)  # indica poucos falsos positivos e falsos negativos, quanto mais próximo de 1, melhor

from sklearn.metrics import confusion_matrix

print("Matriz de Confusão : \n" + str(confusion_matrix(y_test, y_pred))) # Mostrar a matrix de cunfusão, com acertos e erros

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print("True Positive: " + str(tp)) # Verdadeiro positivo
print("True Negative: " + str(tn)) # Verdadeiro Negativo
print("False Positive: " + str(fp)) # Falso positivo
print("False Negative: " + str(fn)) # Falso Negativo

tp / (tp + 27)

# CURVA ROC: calcula fpr e tpr para vários limiares, Verificar a quantidade ce acertos e erros.
from sklearn.metrics import roc_curve
from sklearn.model_selection import cross_val_predict

# probabilidades
# probs = clf_RF.predict_proba(X_test)

y_scores = cross_val_predict(clf_RF, X_test, y_test)

# obtem fpr, tpr e limites
fpr, tpr, thresholds = roc_curve(y_test, y_scores)


def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], "k--")
    plt.axis([0, 1, 0, 1])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")


plot_roc_curve(fpr, tpr)

clf_RF.feature_importances_

#  Importancia das colunas
feature_imp = pd.Series(clf_RF.feature_importances_, index=X_train.columns).sort_values(
    ascending=False
)
feature_imp

# Creating a bar plot // Visualizar a imporancia de cada coluna dentro da rede neural.
_ = plt.figure(figsize=(10, 6))
_ = sns.barplot(x=feature_imp, y=feature_imp.index)

# Add labels to your graph
_ = plt.xlabel("Feature Importance Score")
_ = plt.ylabel("Features")
_ = plt.title("Visualização da Importantancia de cada coluna")
_ = plt.savefig("rf_features.png")

# Melhorando os modelos das arvores de decisão

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, truncnorm, randint

# Tunando os hiperparâmetros:
model_params = {
    # randomly sample numbers from 4 to 204 estimators
    "n_estimators": randint(4, 200),
    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1
    "max_features": truncnorm(a=0, b=1, loc=0.25, scale=0.1),
    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)
    "min_samples_split": uniform(0.01, 0.199),
}

# create random forest classifier model
rf_model = RandomForestClassifier()

# set up random search meta-estimator
# this will train 100 models over 5 folds of cross validation (500 models total)
clf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5, random_state=1) # aplicando 100 vezes para encontrar o melhor resultado de treino

# train the random search meta-estimator to find the best model out of 100 candidates
model = clf.fit(X_train, y_train)

# print winning set of hyperparameters
from pprint import pprint

pprint(model.best_estimator_.get_params())

# Qual combinação de parâmetros trouxe melhor resultado:
model.best_estimator_

# Aplciando treino e teste com os melhores resultados

clf_random = RandomForestClassifier(
    max_features=0.3124639258611636,
    min_samples_split=0.05068599769657197,
    n_estimators=160,
)

clf_random.fit(X_train, y_train)

# Nova taxa de acerto da arvore de decisão // analise a previsão se manteve em 67 % devido a base ser pequena

y_pred_random = clf_random.predict(X_test)

print("Acurácia: " + str(accuracy_score(y_test, y_pred_random)))

print("F1 Score: {}".format(f1_score(y_test, y_pred_random)))